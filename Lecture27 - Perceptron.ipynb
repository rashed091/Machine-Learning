{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\LaTeX \\text{ command declarations here.}\n",
    "\\newcommand{\\N}{\\mathcal{N}}\n",
    "\\newcommand{\\R}{\\mathbb{R}}\n",
    "\\renewcommand{\\vec}[1]{\\mathbf{#1}}\n",
    "\\newcommand{\\norm}[1]{\\|#1\\|_2}\n",
    "\\newcommand{\\d}{\\mathop{}\\!\\mathrm{d}}\n",
    "\\newcommand{\\qed}{\\qquad \\mathbf{Q.E.D.}}\n",
    "\\newcommand{\\vx}{\\mathbf{x}}\n",
    "\\newcommand{\\vy}{\\mathbf{y}}\n",
    "\\newcommand{\\vt}{\\mathbf{t}}\n",
    "\\newcommand{\\vb}{\\mathbf{b}}\n",
    "\\newcommand{\\vw}{\\mathbf{w}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division;\n",
    "import numpy as np;\n",
    "from matplotlib import pyplot as plt;\n",
    "from matplotlib import colors\n",
    "import matplotlib as mpl;\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import mlab;\n",
    "from matplotlib import gridspec;\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "if \"bmh\" in plt.style.available: plt.style.use(\"bmh\");\n",
    "\n",
    "import scipy as scp;\n",
    "\n",
    "from scipy import linalg\n",
    "\n",
    "import scipy.stats;\n",
    "\n",
    "# scikit-learn\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "# python\n",
    "import random;\n",
    "\n",
    "# warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# rise config\n",
    "from notebook.services.config import ConfigManager\n",
    "cm = ConfigManager()\n",
    "cm.update('livereveal', {\n",
    "              'theme': 'simple',\n",
    "              'start_slideshow_at': 'selected',\n",
    "              'transition':'fade',\n",
    "              'scroll': False\n",
    "\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron Algorithm\n",
    "\n",
    "- The Perceptron is the most basic model of a training a linear predictor with sequential update steps:\n",
    "\n",
    "- Given data $\\{\\vx_n, t_n \\}_{n=1}^N$, $t_n \\in \\{-1, 1\\}$, here is how perceptron works:\n",
    "\n",
    "    - **Initialize**: Set $\\vec{w}_1 = \\mathbf{0}$;\n",
    "\n",
    "    - **For:** $n=1,2,\\ldots, N$\n",
    "        - Observe $\\vec{x}_n$, predict $y_n = \\text{sign}(\\vec{w}_t ^{\\top} \\phi(\\vec{x}_n)) $\n",
    "        - Receive $t_n \\in \\{-1,1\\}$, update:\n",
    "            $$\n",
    "            \\vec{w}_{n+1} = \\begin{cases}\n",
    "            \\vec{w}_n & \\text{if } t_n\\vec{w}_n ^T \\phi(\\vec{x}_n) > 0 &\\text{Correct Prediction}\\\\\n",
    "            \\vec{w}_n + t_n \\phi(\\vec{x}_n) & \\mbox{otherwise} & \\text{Incorrect Prediction}\n",
    "            \\end{cases}\n",
    "            $$\n",
    "    - **End**\n",
    "        \n",
    "- Note that we could repeat the for-loop multiple loops until classification error is less than certain threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron: Intuition\n",
    "\n",
    "- We have update\n",
    "    $$\n",
    "    \\vec{w}_{n+1} = \\begin{cases}\n",
    "    \\vec{w}_n & \\text{if } t_n\\vec{w}_n ^T \\phi(\\vec{x}_n) > 0 &\\text{Correct Prediction}\\\\\n",
    "    \\vec{w}_n + t_n \\phi(\\vec{x}_n) & \\mbox{otherwise} & \\text{Incorrect Prediction}\n",
    "    \\end{cases}\n",
    "    $$\n",
    "    \n",
    "- The more *positive* $t_n\\vec{w}_n ^T \\phi(\\vec{x}_n)$ is, the more robust performance $\\vec{w}_n$ has on data $\\vx_n$\n",
    "\n",
    "- **Intuition**\n",
    "    - When $\\vw_n$ gives incorrect prediction for $\\vx_n$, i.e. $t_n\\vec{w}_n ^T \\phi(\\vec{x}_n) \\leq 0$, above update tells us\n",
    "        $$\n",
    "        \\begin{align}\n",
    "        t_n\\vec{w}_{n+1} ^T \\phi(\\vec{x}_n) \n",
    "        &= t_n\\vec{w}_n ^T \\phi(\\vec{x}_n) + t_n^2 \\phi(\\vx_n)^T \\phi(\\vx_n) \\\\\n",
    "        &= t_n\\vec{w}_n ^T \\phi(\\vec{x}_n) + \\underbrace{t_n^2 \\| \\phi(\\vx_n) \\|^2}_{\\text{Non-negative}}\n",
    "        \\end{align}\n",
    "        $$\n",
    "    - **Non-negative** term $t_n^2 \\| \\phi(\\vx_n) \\|^2$ makes $t_n\\vec{w}_{n+1} ^T \\phi(\\vec{x}_n)$ more likely to be positive.\n",
    "    - Therefore, $\\vec{w}_{n+1}$ is likely to have better performance on $\\vx_n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron: Essentially a form of Stochastic Gradient Descent\n",
    "\n",
    "- Define error function:\n",
    "    $$\n",
    "    E(\\vec{w}) = \\sum_{n=1}^N \\max(0, -t_n \\vec{w}^T\\phi(\\vec{x}_n))\n",
    "    $$\n",
    "\n",
    "- The derivative of $E(\\vec{w})$ on data $\\vx_n$ is\n",
    "    $$\n",
    "    \\nabla_\\vw E(\\vec{w} | \\vx_n) = \n",
    "    \\begin{cases}\n",
    "    0 & \\text{if } t_n\\vec{w} ^T \\phi(\\vec{x}_n) > 0 \\\\\n",
    "    -t_n \\phi(\\vec{x}_n) & \\mbox{otherwise} & \n",
    "    \\end{cases}\n",
    "    $$\n",
    "\n",
    "- Perceptron is equivalent to the following stochastic gradient descent\n",
    "    - **For**: $n=1,2,\\ldots, N$\n",
    "        - $\\vec{w}_\\text{new} = \\vec{w}_\\text{old}-\\eta \\nabla_\\vw E(\\vec{w}_{\\text{old}} | \\vx_n)$\n",
    "    - **End**\n",
    "        \n",
    "    \n",
    "- Notice the \"step size\" is $\\eta = 1$! This is atypical.\n",
    "- Perceptron was (originally) viewed as building block of the *neural network* (NN). Indeed, NN often called the Multi-Layer Perceptron (MLP)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron: A magical property\n",
    "\n",
    "- If problem is *linearly separable*, i.e. a hyperplane separates positives/negatives, then Perceptron *will find a separating* $\\vec{w}^*$.\n",
    "\n",
    "- **Theorem**: \n",
    "    * Assume that $\\|\\phi(\\vec{x}_n)\\| \\leq 1$ for all $n$\n",
    "    * Assume $\\exists \\vec{w}$, with $\\|\\vec{w}\\|_2 = 1$, such that for all $(\\vec{x}_n,t_n)$ that $ t_n\\vec{w} ^T \\phi(\\vec{x}_n) > \\gamma$ for some $\\gamma > 0$.\n",
    "    * Then the Perceptron algorithm will find some $\\vec{w}^*$ which perfectly classifies all examples\n",
    "    * The number of updates/mistakes in learning is bounded by $\\frac{1}{\\gamma^2}$\n",
    "    \n",
    "- This is a *margin bound*, notice that it depends on $\\gamma$ not the dimension of $\\phi(\\vec{x})$\n",
    "\n",
    "- Proof is in the notes\n",
    "\n",
    "> Remark\n",
    "\n",
    "> - **Proof Sketch**\n",
    "    > - Let $\\vec{w}_*$ be perfect classifier scaled by $\\frac{1}{\\gamma}$.\n",
    "    $$\n",
    "    \\begin{align}\n",
    "    \\frac{1}{\\gamma^2} = \\norm{\\vec{w}_*}^2 & \\ge \\norm{\\vec{w}_* - \\mathbf 0}^2 - \\norm{\\vec{w}_* - \\vec{w}_{T+1}}^2 \\nonumber \\\\\n",
    "    & = \\sum \\nolimits_{n=1}^T \\norm{\\vec{w}_* - \\vec{w}_{n}}^2 - \\norm{\\vec{w}_* - \\vec{w}_{n+1}}^2  \\\\\n",
    "    & = \\sum \\nolimits_{n \\, : \\, t_n \\vec{w}_n^T \\phi(\\vec{x}_n) < 0 } \\norm{\\vec{w}_* - \\vec{w}_{n}}^2 - \\norm{\\vec{w}_* - (\\vec{w}_n + t_n \\phi(\\vec{x}_n)}^2 \\\\\n",
    "    & = \\sum \\nolimits_{n \\, : \\, t_n \\vec{w}_n^T \\phi(\\vec{x}_n) < 0} 2 \\left( \\underbrace{t_n (\\vec{w}_*^T \\phi(\\vec{x}_n))}_{\\ge 1} \\underbrace{- t_n (\\vec{w}_n^T \\phi(\\vec{x}_n))}_{\\ge 0} \\right) \\underbrace{- t_n^2 \\norm{\\phi(\\vec{x}_n)}^2}_{\\ge -1} \\\\\n",
    "    & \\ge \\sum \\nolimits_{n \\, : \\, t_n \\vec{w}_n^T \\phi(\\vec{x}_n) < 0} 1 \\quad = \\quad \\text{#mistakes[Perceptron]}\n",
    "    \\end{align}\n",
    "    $$\n",
    "\n",
    "> - See [learning theory lecture notes](http://web.eecs.umich.edu/~jabernet/eecs598course/fall2015/web/notes/lec16_110515.pdf) for full details. (Note that we have changed notations a little bit. Index $n$, label $t_n$ and data feature vector $\\phi(\\vx)$ each corresponds to index $t$, label $y_n$ and data $\\vx$ in this reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
