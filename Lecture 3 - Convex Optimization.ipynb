{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lecture 3: Convex Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from IPython.core.display import HTML, Image\n",
    "from IPython.display import YouTubeVideo\n",
    "from sympy import init_printing, Matrix, symbols, Rational\n",
    "import sympy as sym\n",
    "from warnings import filterwarnings\n",
    "init_printing(use_latex = 'mathjax')\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from notebook.services.config import ConfigManager\n",
    "cm = ConfigManager()\n",
    "cm.update('livereveal', {\n",
    "              'theme': 'simple',\n",
    "              'start_slideshow_at': 'selected',\n",
    "})\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Outline for this Lecture\n",
    "- Convexity\n",
    "    - Convex Set\n",
    "    - Convex Function\n",
    "- Introduction to Optimization\n",
    "- Introduction to Lagrange Duality\n",
    "* Unconstrained Optimization\n",
    "* Constrained Optimization\n",
    "    * Langrange duality\n",
    "    * KKT conditions\n",
    "    * Convex function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "> In this lecture, we will first introduce convex set, convex function and optimization problem. One approach to solve optimization problem is to solve its dual problem. We will briefly cover some basics of duality in this lecture. More about optimization and duality will come when we study support vector machine (SVM)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Convex Sets\n",
    "- $C \\subseteq \\mathbb{R}^n$ is **convex** if\n",
    "    $$t x + (1-t)y \\in C$$\n",
    "    for any $x, y \\in C$ and $0 \\leq t \\leq 1$\n",
    "- that is, a set is convex if the line connecting **any** two points in the set is entirely inside the set\n",
    "<center>\n",
    "<div class=\"image\" style=\"width:450px\">\n",
    "  <img src=\"images/Convex_Set.png\">\n",
    "</div>\n",
    "</center>\n",
    "<center><span>(Left: Convex Set; Right: Non-convex Set)</span></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Convex Functions\n",
    "* We say that a function $f$ is *convex* if, for any distinct pair of points $x,y$ we have\n",
    "    $$\n",
    "    f(tx_1+(1-t)x_2) \\leq tf(x_1)+(1-t)f(x_2) \\quad \\forall t \\in[0,1]\n",
    "    $$\n",
    "* A function $f$ is said to be *concave* if $-f$ is convex\n",
    "<center>\n",
    "<div class=\"image\" style=\"width:500px\">\n",
    "  <img src=\"images/Convex_Fun.png\">\n",
    "</div>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Fun Facts About Convex Functions\n",
    "* If $f$ is differentiable, then $f$ is convex iff $f$ \"lies above its linear approximation\", i.e.:\n",
    "    $$\n",
    "    f(x + y) \\geq f(x) + \\nabla_x f(x) \\cdot y \\quad \\forall x,y\n",
    "    $$\n",
    "* If $f$ is twice-differentiable, then the hessian is always positive semi-definite!\n",
    "* This last one you will show on your homework :-)\n",
    "\n",
    "### For unconstrained optimization\n",
    "\n",
    "* **Stationary point $\\Rightarrow$ global minimizer**\n",
    "* **Uniqueness of global minimizer for strictly convex function** (HW1 Q5)\n",
    "\n",
    "### For constrained optimization\n",
    "* For convex problem\n",
    "    * Use Slater's conditions to get strong duality\n",
    "    * Use KKT conditions to get strong duality and find the solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction to Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Most General Optimization Problem\n",
    "- Assume $f$ is some function, and $C \\subset \\mathbb{R}^n$ is some set. The following is an *optimization problem*:\n",
    "    $$\n",
    "    \\begin{array}{ll}\n",
    "    \\mbox{minimize} & f(x) \\\\\n",
    "    \\mbox{subject to} & x \\in C\n",
    "    \\end{array}\n",
    "    $$\n",
    "-  How hard is it to find a solution that is (near-) optimal? This is one of the fundamental problems in Computer Science and Operations Research.\n",
    "- A huge portion of ML relies on this task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A Rough Optimization Hierarchy\n",
    "- For optimization problem\n",
    "    $$\n",
    "    \\mbox{minimize } \\ f(x) \\quad \\mbox{subject to } x \\in C\n",
    "    $$\n",
    "* **[Really Easy]** $C = \\mathbb{R}^n$ (i.e. problem is *unconstrained*), $f$ is convex, $f$ is differentiable, strictly convex, and \"slowly-changing\" gradients\n",
    "* **[Easyish]** $C = \\mathbb{R}^n$, $f$ is convex\n",
    "* **[Medium]** $C$ is a convex set, $f$ is convex\n",
    "* **[Hard]** $C$ is a convex set, $f$ is non-convex\n",
    "* **[REALLY Hard]** $C$ is an arbitrary set, $f$ is non-convex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Optimization Without Constraints\n",
    "- Optimization problem without constraint is given by\n",
    "    $$\n",
    "    \\begin{array}{ll}\n",
    "    \\mbox{minimize} & f(x) \\\\\n",
    "    \\mbox{subject to} & x \\in \\mathbb{R}^n\n",
    "    \\end{array}\n",
    "    $$\n",
    "- This problem tends to be easier than constrained optimization\n",
    "- We just need to find an $x$ such that $\\nabla f(x) = \\vec{0}$\n",
    "- Techniques like *gradient descent* or *Newton's method* work in this setting. (More on this later)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Optimization With Constraints\n",
    "- Optimization problem with constraint is given by\n",
    "    $$\n",
    "    \\begin{aligned}\n",
    "    & {\\text{minimize}} & & f(\\mathbf{x})\\\\\n",
    "    & \\text{subject to} & & g_i(\\mathbf{x}) \\leq 0, \\quad i = 1, ..., m\\\\\n",
    "    & & & h_j(x) = 0, \\quad j = 1, ..., n\n",
    "    \\end{aligned}\n",
    "    $$\n",
    "- Here $C = \\{ x : g_i(x) \\leq 0,\\ h_j(x) = 0, \\ i=1, \\ldots, m,\\ j = 1, ..., n \\}$\n",
    "- $C$ is convex as long as all $g_i(x)$ convex and all $h_j(x)$ affine\n",
    "- The solution of this optimization may occur in the *interior* of $C$, in which case the optimal $x$ will have $\\nabla f(x) = 0$\n",
    "- But what if the solution occurs on the *boundary* of $C$?\n",
    "<center>\n",
    "<div class=\"image\" style=\"width:500px\">\n",
    "  <img src=\"images/lagrange.gif\">\n",
    "</div>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction to Lagrange Duality\n",
    "- In some cases original (**primal**) optimization problem can hard to solve, solving a proxy problem sometimes can be easier\n",
    "- The proxy problem could be **dual** problem which is transformed from primal problem\n",
    "- Here is how to transform from primal to dual. For primal problem\n",
    "    $$\n",
    "    \\begin{aligned}\n",
    "    & {\\text{minimize}} & & f(\\mathbf{x})\\\\\n",
    "    & \\text{subject to} & & g_i(\\mathbf{x}) \\leq 0, \\quad i = 1, ..., m\\\\\n",
    "    & & & h_j(x) = 0, \\quad j = 1, ..., n\n",
    "    \\end{aligned}\n",
    "    $$\n",
    "    Its Lagrangian is \n",
    "    $$\n",
    "    L(x,\\boldsymbol{\\lambda}, \\boldsymbol{\\nu}) := f(x) + \\sum_{i=1}^m \\lambda_i g_i(x) + \\sum_{j=1}^n \\nu_j h_j(x)\n",
    "    $$\n",
    "    of which $\\boldsymbol{\\lambda} \\in \\mathbb{R}^m$, $\\boldsymbol{\\nu} \\in \\mathbb{R}^n$ are **dual variables**\n",
    "\n",
    "- The **Langrangian dual function** is \n",
    "    $$\n",
    "    L_D(\\boldsymbol{\\lambda}, \\boldsymbol{\\nu}) \\triangleq \\underset{x}{\\inf}L(x,\\boldsymbol{\\lambda}, \\boldsymbol{\\nu}) = \\underset{x}{\\inf} \\ \\left[ f(x) + \\sum_{i=1}^m \\lambda_i g_i(x) + \\sum_{j=1}^n \\nu_j h_j(x) \\right] \n",
    "    $$\n",
    "- The minimization is usually done by finding the stable point of $L(x,\\boldsymbol{\\lambda}, \\boldsymbol{\\nu})$ with respect to $x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Lagrange Dual Problem\n",
    "\n",
    "* Then the **dual problem** is \n",
    "    $$\n",
    "    \\begin{aligned}\n",
    "    & {\\text{maximize}} & & L_D(\\mathbf{\\lambda}, \\mathbf{\\nu})\\\\\n",
    "    & \\text{subject to} & & \\lambda_i, \\nu_j \\geq 0 \\quad i = 1, \\ldots, m ,\\ j = 1, ..., n\\\\\n",
    "    \\end{aligned}\n",
    "    $$\n",
    "- Instead of solving primal problem with respect to $x$, we now need to solve dual problem with respect to $\\mathbf{\\lambda}$ and $\\mathbf{\\nu}$\n",
    "    * $ L_D(\\mathbf{\\lambda}, \\mathbf{\\nu})$ is concave even if primal problem is not convex\n",
    "    * Let the $p^*$ and $d^*$ denote the optimal values of primal problem and dual problem, we always have *weak duality*: $p^* \\geq d^*$\n",
    "    * Under nice conditions, we get *strong duality*: $p^* = d^*$\n",
    "    * Many details are omitted here and they will come when we study **support vector machine (SVM)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unconstrained Optimization\n",
    "\n",
    "* ** Differentiable **: $\\forall x$, the gradient $\\nabla f(x)$ exists\n",
    "* ** Twice differentiable **: $\\forall x$, the Hessian matrix $\\nabla^2 f(x)$ exists\n",
    "* **Stationary point**: $x$ where $\\nabla f(x) = \\vec{0}$\n",
    "* ** Saddle point **: a stationary point but not a local minimizer/maximizer\n",
    "    * e.g. $x=0$ for $f(x) = x^3$\n",
    "\n",
    "\n",
    "* **Problem formulation**\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\min_{x\\in \\mathbb{R}^d} \\quad& f(x)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "* We need to find the stationary points\n",
    "* Global minimizer $\\Rightarrow$ local minimizer $\\Rightarrow$ stationary point\n",
    "* Solving the equation: closed-form solutions, gradient descent, Newton's method, etc.\n",
    "**Stationary point $\\Rightarrow$ local minimizer $\\Rightarrow$ global minimizer?**\n",
    "\n",
    "* Finite # of stationary points: check everyone\n",
    "* Differentiable convex function: global minimizer! $$f(y)\\geq f(x)+\\langle\\nabla f(x),y-x\\rangle=f(x)$$\n",
    "* Twice continuous differentiable function: \n",
    "    $$\n",
    "    \\begin{align*}\n",
    "    f(y) &= f(x) + \\langle\\nabla f(x), y-x\\rangle + \\frac{1}{2} \\langle y-x, \\nabla^2 f(x)(y-x) \\rangle \\\\&+ o(\\Vert y-x\\Vert^2)\\end{align*}$$\n",
    "    * $f(x+tv) - f(x) = t^2\\left(\\frac{1}{2}\\langle v, \\nabla^2 f(x)v \\rangle + \\frac{o(t^2)}{t^2}\\right)$ where we let $\\Vert v\\Vert=1$ ($v$ on the unit ball centered at $x$)\n",
    "    * $\\nabla^2 f(x)$ positive definite $\\Rightarrow$ local minimizer, how about PSD?\n",
    "   \n",
    "* ** Property**: differentiable, local minimizer $x^*$ $\\Rightarrow$ $\\nabla f(x) = \\vec{0}$ (*the inverse is not true*)\n",
    "* ** Property**: twice differentiable, local minimizer $x^*$ $\\Rightarrow$ $\\nabla^2 f(x)$ is PSD (*the inverse is not true*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constrained Optimization\n",
    "\n",
    "* **Problem formulation**\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\min_{x\\in \\mathbb{R}^d} \\quad& f(x) \\\\\n",
    "\\text{s.t.} \\quad& g_i(x) \\leq 0, \\quad i = 1, \\ldots, m\\\\\n",
    " & h_j(x) = 0, \\quad j = 1, \\ldots, n\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "* The set $C=\\{ x\\vert g_i(x) \\leq 0, h_j(x) = 0, i=1, \\ldots, m, j=1, \\ldots, n \\}$ is convex if $g_i(x)$ convex and $h_j(x)$ affine\n",
    "* The solution of this optimization may occur in the *interior* of $C$, in which case the optimal $x$ will have $\\nabla f(x) = 0$\n",
    "* But what if the solution occurs on the *boundary* of $C$?\n",
    "\n",
    "### Langrange Duality\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\min_{x\\in \\mathbb{R}^d} \\quad& f(x) \\\\\n",
    "\\text{s.t.} \\quad& g_i(x) \\leq 0, \\quad i = 1, \\ldots, m\\\\\n",
    "& h_j(x) = 0, \\quad j = 1, \\ldots, n\n",
    "\\end{align*}\n",
    "$$\n",
    "* **Lagrangian function**\n",
    "$$\n",
    "L(x,\\boldsymbol{\\lambda}, \\boldsymbol{\\nu}) := f(x) + \\sum_{i=1}^m \\lambda_i g_i(x) + \\sum_{j=1}^n \\nu_j h_j(x)\n",
    "$$\n",
    "* **Lagrange multipliers/dual variables**: $\\boldsymbol{\\lambda} \\in \\mathbb{R}^m$ and $\\boldsymbol{\\nu} \\in \\mathbb{R}^n$ \n",
    "\n",
    "* $$\n",
    "L(x,\\boldsymbol{\\lambda}, \\boldsymbol{\\nu}) := f(x) + \\sum_{i=1}^m \\lambda_i g_i(x) + \\sum_{j=1}^n \\nu_j h_j(x)\n",
    "$$\n",
    "\n",
    "\n",
    "* **Primal function**\n",
    "$$\n",
    "\\begin{align*}\n",
    "L_{P}(x)=\n",
    "\\left\\{\\begin{array}{r} \\max_{\\boldsymbol{\\lambda}, \\boldsymbol{\\nu}}\\, L(x,\\boldsymbol{\\lambda}, \\boldsymbol{\\nu}) \\\\ \\text{s.t.}\\quad \\lambda_i\\geq 0\\end{array}\\right.\n",
    "=\\left\\{\\begin{array}{ll}f(x) & x\\in C\\\\ +\\infty & x\\notin C \\end{array}\\right.\n",
    "\\end{align*}\n",
    "$$\n",
    "where $ C = \\{ x\\vert g_i(x) \\leq 0, h_j(x) = 0, i=1, \\ldots, m, j=1, \\ldots, n \\}$\n",
    "\n",
    "\n",
    "* What's the difference between $f(x)$ and $L_P(x)$?\n",
    "\n",
    "Hence the original optimization is equivalent to the **primal problem**:\n",
    "$$\\min_{x\\in \\mathbb{R}^n} L_P(x)$$\n",
    "or\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\min_{x\\in \\mathbb{R}^d} \\max_{\\boldsymbol{\\lambda}\\in \\mathbb{R}^m, \\boldsymbol{\\nu}\\in \\mathbb{R}^n}\\, &L(x,\\boldsymbol{\\lambda}, \\boldsymbol{\\nu})\\\\\n",
    "\\text{s.t.}\\quad &\\lambda_i \\geq 0\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "* It is actually a double optimization (inner optimization in exchange of unconstrained $x$)\n",
    "\n",
    "* Swap the outer and inner optimization to get the **dual problem**:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\max_{\\boldsymbol{\\lambda}\\in \\mathbb{R}^m, \\boldsymbol{\\nu}\\in \\mathbb{R}^n}\\min_{x\\in \\mathbb{R}^d}\\, &L(x,\\boldsymbol{\\lambda}, \\boldsymbol{\\nu})\\\\\n",
    "\\text{s.t.}\\quad &\\lambda_i \\geq 0\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "** Dual function **\n",
    "$$L_D(\\boldsymbol{\\lambda}\\in \\mathbb{R}^m, \\boldsymbol{\\nu}\\in \\mathbb{R}^n)=\\min_{x\\in \\mathbb{R}^d}\\, L(x,\\boldsymbol{\\lambda}, \\boldsymbol{\\nu})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the connection between primal and dual problem?\n",
    "* Primal solution: $x^*$, dual solution: $\\lambda^*$ and $\\nu^*$\n",
    "\n",
    "* **Weak duality** (always true)\n",
    "$$d^*=L_D(\\boldsymbol{\\lambda}^*, \\boldsymbol{\\nu}^*)\\leq p^*=L_P(x^*)=f(x^*)$$\n",
    "\n",
    "* **Strong duality** (under additional conditions): $d^* = p^* =f(x^*)= L(x^*,\\boldsymbol{\\lambda}^*, \\boldsymbol{\\nu}^*)$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "L(x^*,\\boldsymbol{\\lambda}^*, \\boldsymbol{\\nu}^*)\n",
    "\\leq p^*=\\max_{\\boldsymbol{\\lambda}, \\boldsymbol{\\nu}}\\, L(x^*,\\boldsymbol{\\lambda}, \\boldsymbol{\\nu})\n",
    "\\leq d^*=\\min_x\\, L(x,\\boldsymbol{\\lambda}^*, \\boldsymbol{\\nu}^*)\\leq L(x^*,\\boldsymbol{\\lambda}^*,\\boldsymbol{\\nu}^*)\n",
    "\\end{align*}\n",
    "$$\n",
    "* Sometimes the dual problem is easier. But when strong duality holds?\n",
    "\n",
    "### Karush–Kuhn–Tucker (KKT) conditions\n",
    "* Suppose differentiable $f(x),g_i(x),h_j(x)$\n",
    "* (Primal feasibility) $$g_i(x)\\leq 0, h_j(x)=0$$\n",
    "* (Dual feasibility) $$\\lambda_i\\geq 0$$\n",
    "* (Stationarity) $$\\nabla_x L(x, \\boldsymbol{\\lambda}, \\boldsymbol{\\nu})=0$$\n",
    "* (Complementary slackness) $$\\lambda_i g_i(x)=0$$\n",
    "\n",
    "### Necessary conditions of strong duality\n",
    "* Strong duality $\\Rightarrow$ KKT conditions hold for $x^*$, $\\lambda^*$ and $\\nu^*$ (Proof?)\n",
    "\n",
    "\n",
    "### Sufficient conditions of strong duality\n",
    "* KKT conditions hold for $\\tilde{x}$, $\\tilde{\\boldsymbol{\\lambda}}$ and $\\tilde{\\boldsymbol{\\nu}}$ $\\Rightarrow$ strong duality, primal solution $\\tilde{x}$, dual solution $\\tilde{\\boldsymbol{\\lambda}}$ and $\\tilde{\\boldsymbol{\\nu}}$ (Proof?)\n",
    "* For convex problem: $f(x)$ and $g_i(x)$ are convex, $h_j(x)$ are affine\n",
    "* Slater's conditions: $\\exists x$ s.t. $g_i(x)<0$ and $h_j(x)=0$\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommended reading:\n",
    "* Free online!\n",
    "* Chapter 5 covers duality\n",
    "<center>\n",
    "<div class=\"image\" style=\"width:200px\">\n",
    "  <img src=\"images/bv_cvxbook_cover.jpg\">\n",
    "</div>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
