{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax regression\n",
    "Softmax regression (or multinomial logistic regression) is a generalization of logistic regression to the case where we want to handle multiple classes. In logistic regression we assumed that the labels were binary: $y^{(i)} \\in \\{0,1\\}$. We used such a classifier to distinguish between two kinds of hand-written digits. Softmax regression allows us to handle $y^{(i)} \\in \\{1,\\ldots,K\\}$ where $K$ is the number of classes.\n",
    "\n",
    "Recall that in logistic regression, we had a training set $\\{ (x^{(1)}, y^{(1)}), \\ldots, (x^{(m)}, y^{(m)}) \\}$ of $m$ labeled examples, where the input features are $x^{(i)} \\in \\Re^{n}$. With logistic regression, we were in the binary classification setting, so the labels were $y^{(i)} \\in \\{0,1\\}$. Our hypothesis took the form:\n",
    "\\begin{align*}\n",
    "h_\\theta(x) = \\frac{1}{1+\\exp(-\\theta^\\top x)},\n",
    "\\end{align*}\n",
    "and the model parameters $\\theta$ were trained to minimize the cost function\n",
    "\\begin{align*}\n",
    "J(\\theta) = -\\left[ \\sum_{i=1}^m y^{(i)} \\log h_\\theta(x^{(i)}) + (1-y^{(i)}) \\log (1-h_\\theta(x^{(i)})) \\right]\n",
    "\\end{align*}\n",
    "In the softmax regression setting, we are interested in multi-class classification (as opposed to only binary classification), and so the label $y$ can take on $K$ different values, rather than only two. Thus, in our training set $\\{ (x^{(1)}, y^{(1)}), \\ldots, (x^{(m)}, y^{(m)}) \\}$, we now have that $y^{(i)} \\in \\{1, 2, \\ldots, K\\}$. (Note that our convention will be to index the classes starting from 1, rather than from 0.) For example, in the MNIST digit recognition task, we would have $K=10$ different classes.\n",
    "\n",
    "Given a test input $x$, we want our hypothesis to estimate the probability that $P(y=k | x)$ for each value of $k = 1, \\ldots, K$. I.e., we want to estimate the probability of the class label taking on each of the $K$ different possible values. Thus, our hypothesis will output a $K$-dimensional vector (whose elements sum to 1) giving us our $K$ estimated probabilities. Concretely, our hypothesis $h_{\\theta}(x)$ takes the form:\n",
    "\\begin{align*}\n",
    "h_\\theta(x) =\n",
    "\\begin{bmatrix}\n",
    "P(y = 1 | x; \\theta) \\\\\n",
    "P(y = 2 | x; \\theta) \\\\\n",
    "\\vdots \\\\\n",
    "P(y = K | x; \\theta)\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\frac{1}{ \\sum_{j=1}^{K}{\\exp(\\theta^{(j)\\top} x) }}\n",
    "\\begin{bmatrix}\n",
    "\\exp(\\theta^{(1)\\top} x ) \\\\\n",
    "\\exp(\\theta^{(2)\\top} x ) \\\\\n",
    "\\vdots \\\\\n",
    "\\exp(\\theta^{(K)\\top} x ) \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "Here $\\theta^{(1)}, \\theta^{(2)}, \\ldots, \\theta^{(K)} \\in \\Re^{n}$ are the parameters of our model. Notice that the term $\\frac{1}{ \\sum_{j=1}^{K}{\\exp(\\theta^{(j)\\top} x) } }$ normalizes the distribution, so that it sums to one.\n",
    "\n",
    "For convenience, we will also write $\\theta$ to denote all the parameters of our model. When you implement softmax regression, it is usually convenient to represent $\\theta$ as a $n$-by-$K$ matrix obtained by concatenating $\\theta^{(1)}, \\theta^{(2)}, \\ldots, \\theta^{(K)}$ into columns, so that\n",
    "\\begin{align*}\n",
    "\\theta = \\left[\\begin{array}{cccc}| & | & | & | \\\\\n",
    "\\theta^{(1)} & \\theta^{(2)} & \\cdots & \\theta^{(K)} \\\\\n",
    "| & | & | & |\n",
    "\\end{array}\\right].\n",
    "\\end{align*}\n",
    "\n",
    "### Cost Function\n",
    "We now describe the cost function that we’ll use for softmax regression. In the equation below, $1\\{\\cdot\\}$ is the ”‘indicator function,”’ so that $1\\{\\hbox{a true statement}\\}=1$, and $1\\{\\hbox{a false statement}\\}=0$. For example, $1\\{2+2=4\\}$ evaluates to 1; whereas $1\\{1+1=5\\}$ evaluates to 0. Our cost function will be:\n",
    "\\begin{align*}\n",
    "J(\\theta) = - \\left[ \\sum_{i=1}^{m} \\sum_{k=1}^{K}  1\\left\\{y^{(i)} = k\\right\\} \\log \\frac{\\exp(\\theta^{(k)\\top} x^{(i)})}{\\sum_{j=1}^K \\exp(\\theta^{(j)\\top} x^{(i)})}\\right]\n",
    "\\end{align*}\n",
    "Notice that this generalizes the logistic regression cost function, which could also have been written:\n",
    "\\begin{align*}\n",
    "J(\\theta) &= - \\left[ \\sum_{i=1}^m   (1-y^{(i)}) \\log (1-h_\\theta(x^{(i)})) + y^{(i)} \\log h_\\theta(x^{(i)}) \\right] \\\\\n",
    "&= - \\left[ \\sum_{i=1}^{m} \\sum_{k=0}^{1} 1\\left\\{y^{(i)} = k\\right\\} \\log P(y^{(i)} = k | x^{(i)} ; \\theta) \\right]\n",
    "\\end{align*}\n",
    "The softmax cost function is similar, except that we now sum over the $K$ different possible values of the class label. Note also that in softmax regression, we have that $P(y^{(i)} = k | x^{(i)} ; \\theta) = \\frac{\\exp(\\theta^{(k)\\top} x^{(i)})}{\\sum_{j=1}^K \\exp(\\theta^{(j)\\top} x^{(i)}) }$\n",
    "We cannot solve for the minimum of $J(\\theta)$ analytically, and thus as usual we’ll resort to an iterative optimization algorithm. Taking derivatives, one can show that the gradient is:\n",
    "\\begin{align*}\n",
    "\\nabla_{\\theta^{(k)}} J(\\theta) = - \\sum_{i=1}^{m}{ \\left[ x^{(i)} \\left( 1\\{ y^{(i)} = k\\}  - P(y^{(i)} = k | x^{(i)}; \\theta) \\right) \\right]  }\n",
    "\\end{align*}\n",
    "Recall the meaning of the $\\nabla_{\\theta^{(k)}}$ notation. In particular, $\\nabla_{\\theta^{(k)}} J(\\theta)$ is itself a vector, so that its $j^{th}$ element is $\\frac{\\partial J(\\theta)}{\\partial \\theta_{lk}}$ the partial derivative of $J(\\theta)$ with respect to the $j^{th}$ element of $\\theta^{(k)}$.\n",
    "\n",
    "Armed with this formula for the derivative, one can then plug it into a standard optimization package and have it minimize $J(\\theta)$. When implementing softmax regression, we will typically use a modified version of the cost function described above;\n",
    "specifically, one that incorporates weight decay.  We describe the motivation and details below.\n",
    "\n",
    "### Properties of softmax regression parametrization\n",
    "Softmax regression has an unusual property that it has a “redundant” set of parameters. To explain what this means, suppose we take each of our parameter vectors $\\theta^{(j)}$, and subtract some fixed vector $\\psi$ from it, so that every $\\theta^{(j)}$ is now replaced with $\\theta^{(j)} - \\psi$ (for every $j=1, \\ldots, k$). Our hypothesis now estimates the class label probabilities as.\n",
    "\\begin{align*}\n",
    "P(y^{(i)} = k | x^{(i)} ; \\theta)\n",
    "&= \\frac{\\exp((\\theta^{(k)}-\\psi)^\\top x^{(i)})}{\\sum_{j=1}^K \\exp( (\\theta^{(j)}-\\psi)^\\top x^{(i)})}  \\\\\n",
    "&= \\frac{\\exp(\\theta^{(k)\\top} x^{(i)}) \\exp(-\\psi^\\top x^{(i)})}{\\sum_{j=1}^K \\exp(\\theta^{(j)\\top} x^{(i)}) \\exp(-\\psi^\\top x^{(i)})} \\\\\n",
    "&= \\frac{\\exp(\\theta^{(k)\\top} x^{(i)})}{\\sum_{j=1}^K \\exp(\\theta^{(j)\\top} x^{(i)})}.\n",
    "\\end{align*}\n",
    "In other words, subtracting $\\psi$ from every $\\theta^{(j)}$ does not affect our hypothesis’ predictions at all! This shows that softmax regression’s parameters are “redundant.” More formally, we say that our softmax model is ”‘overparameterized,”’ meaning that for any hypothesis we might fit to the data, there are multiple parameter settings that give rise to exactly the same hypothesis function $h_\\theta$ mapping from inputs $x$ to the predictions.\n",
    "\n",
    "Further, if the cost function $J(\\theta)$ is minimized by some setting of the parameters $(\\theta^{(1)}, \\theta^{(2)},\\ldots, \\theta^{(k)})$, then it is also minimized by $(\\theta^{(1)} - \\psi, \\theta^{(2)} - \\psi,\\ldots,\n",
    "\\theta^{(k)} - \\psi)$ for any value of $\\psi$. Thus, the minimizer of $J(\\theta)$ is not unique. (Interestingly, $J(\\theta)$ is still convex, and thus gradient descent will not run into local optima problems. But the Hessian is singular/non-invertible, which causes a straightforward implementation of Newton’s method to run into numerical problems.)\n",
    "\n",
    "Notice also that by setting $\\psi = \\theta^{(K)}$, one can always replace $\\theta^{(K)}$ with $\\theta^{(K)} - \\psi = \\vec{0}$ (the vector of all 0’s), without affecting the hypothesis. Thus, one could “eliminate” the vector of parameters $\\theta^{(K)}$ (or any other $\\theta^{(k)}$, for any single value of $k$), without harming the representational power of our hypothesis. Indeed, rather than optimizing over the $K\\cdot n$ parameters $(\\theta^{(1)}, \\theta^{(2)},\\ldots, \\theta^{(K)})$ (where $\\theta^{(k)} \\in \\Re^{n}$), one can instead set $\\theta^{(K)} = \\vec{0}$ and optimize only with respect to the $K \\cdot n$ remaining parameters. In practice, however, it is often cleaner and simpler to implement the version which keeps\n",
    "all the parameters $(\\theta^{(1)}, \\theta^{(2)},\\ldots, \\theta^{(n)})$, without\n",
    "arbitrarily setting one of them to zero.  But we will\n",
    "make one change to the cost function: Adding weight decay.  This will take care of\n",
    "the numerical problems associated with softmax regression's overparameterized representation.\n",
    "\n",
    "### Weight Decay\n",
    "We will modify the cost function by adding a weight decay term \n",
    "$\\textstyle \\frac{\\lambda}{2} \\sum_{i=1}^k \\sum_{j=0}^{n} \\theta_{ij}^2$\n",
    "which penalizes large values of the parameters.  Our cost function is now\n",
    "\\begin{align*}\n",
    "J(\\theta) = - \\frac{1}{m} \\left[ \\sum_{i=1}^{m} \\sum_{j=1}^{k} 1\\left\\{y^{(i)} = j\\right\\} \\log \\frac{e^{\\theta^{(j)\\top} x^{(i)}}}{\\sum_{l=1}^k e^{ \\theta^{(l)\\top} x^{(i)} }}  \\right]\n",
    "              + \\frac{\\lambda}{2} \\sum_{i=1}^k \\sum_{j=0}^n \\theta_{ij}^2\n",
    "\\end{align*}\n",
    "\n",
    "With this weight decay term (for any $\\lambda > 0$), the cost function\n",
    "$J(\\theta)$ is now strictly convex, and is guaranteed to have a\n",
    "unique solution.  The Hessian is now invertible, and because $J(\\theta)$ is \n",
    "convex, algorithms such as gradient descent, L-BFGS, etc. are guaranteed\n",
    "to converge to the global minimum. To apply an optimization algorithm, we also need the derivative of this\n",
    "new definition of $J(\\theta)$.  One can show that the derivative is:\n",
    "\\begin{align*}\n",
    "\\nabla_{\\theta^{(j)}} J(\\theta) = - \\frac{1}{m} \\sum_{i=1}^{m}{ \\left[ x^{(i)} ( 1\\{ y^{(i)} = j\\}  - P(y^{(i)} = j | x^{(i)}; \\theta) ) \\right]  } + \\lambda \\theta^{(j)}\n",
    "\\end{align*}\n",
    "\n",
    "By minimizing $J(\\theta)$ with respect to $\\theta$, we will have a working implementation of softmax regression.\n",
    "\\subsection{Relationship to Logistic Regression}\n",
    "In the special case where $K = 2$, one can show that softmax regression reduces to logistic regression. This shows that softmax regression is a generalization of logistic regression. Concretely, when $K=2$, the softmax regression hypothesis outputs\n",
    "\\begin{align*}\n",
    "h_\\theta(x) &=\n",
    "\\frac{1}{ \\exp(\\theta^{(1)\\top}x)  + \\exp( \\theta^{(2)\\top} x^{(i)} ) }\n",
    "\\begin{bmatrix}\n",
    "\\exp( \\theta^{(1)\\top} x ) \\\\\n",
    "\\exp( \\theta^{(2)\\top} x )\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "Taking advantage of the fact that this hypothesis is overparameterized and setting $\\psi = \\theta^{(2)}$, we can subtract $\\theta^{(2)}$ from each of the two parameters, giving us\n",
    "\\begin{align*}\n",
    "h(x) &=\n",
    "\\frac{1}{ \\exp( (\\theta^{(1)}-\\theta^{(2)})^\\top x^{(i)} ) + \\exp(\\vec{0}^\\top x) }\n",
    "\\begin{bmatrix}\n",
    "\\exp( (\\theta^{(1)}-\\theta^{(2)})^\\top x )\n",
    "\\exp( \\vec{0}^\\top x ) \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "\\frac{1}{ 1 + \\exp( (\\theta^{(1)}-\\theta^{(2)})^\\top x^{(i)} ) } \\\\\n",
    "\\frac{\\exp( (\\theta^{(1)}-\\theta^{(2)})^\\top x )}{ 1 + \\exp( (\\theta^{(1)}-\\theta^{(2)})^\\top x^{(i)} ) }\n",
    "\\end{bmatrix} \\\\\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "\\frac{1}{ 1  + \\exp( (\\theta^{(1)}-\\theta^{(2)})^\\top x^{(i)} ) } \\\\\n",
    "1 - \\frac{1}{ 1  + \\exp( (\\theta^{(1)}-\\theta^{(2)})^\\top x^{(i)} ) } \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "Thus, replacing $\\theta^{(2)}-\\theta^{(1)}$ with a single parameter vector $\\theta'$, we find that softmax regression predicts the probability of one of the classes as $\\frac{1}{ 1  + \\exp(- (\\theta')^\\top x^{(i)} ) }$, and that of the other class as $1 - \\frac{1}{ 1 + \\exp(- (\\theta')^\\top x^{(i)} ) }$, same as logistic regression.\n",
    "\n",
    "### Relationship to Logistic Regression\n",
    "In the special case where k = 2, one can show that softmax regression reduces to logistic regression. This shows that softmax regression is a generalization of logistic regression. Concretely, when k = 2, the softmax regression hypothesis outputs\n",
    "\n",
    "\\begin{align*}\n",
    "h_\\theta(x) &= \\frac{1}{ e^{\\theta_1^Tx}  + e^{ \\theta_2^T x^{(i)} } }\n",
    "\\begin{bmatrix}\n",
    "&e^{ \\theta_1^T x } \\\\&e^{ \\theta_2^T x }\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "Taking advantage of the fact that this hypothesis is overparameterized and setting ψ = θ1, we can subtract θ1 from each of the two parameters, giving us\n",
    "\n",
    "\n",
    "Thus, replacing θ2 − θ1 with a single parameter vector θ', we find that softmax regression predicts the probability of one of the classes as $$\\frac{1}{ 1  + e^{ (\\theta')^T x^{(i)} } }$$, and that of the other class as $$1 - \\frac{1}{ 1 + e^{ (\\theta')^T x^{(i)} } }$$, same as logistic regression.\n",
    "\n",
    "\n",
    "### Softmax Regression vs. $k$ Binary Classifiers\n",
    "Suppose you are working on a music classification application, and there are\n",
    "$k$ types of music that you are trying to recognize.  Should you use a\n",
    "softmax classifier, or should you build $k$ separate binary classifiers using\n",
    "logistic regression?\n",
    "\n",
    "This will depend on whether the four classes are ''mutually exclusive.''  For example,\n",
    "if your four classes are classical, country, rock, and jazz, then assuming each\n",
    "of your training examples is labeled with exactly one of these four class labels,\n",
    "you should build a softmax classifier with $k=4$.\n",
    "(If there're also some examples that are none of the above four classes,\n",
    "then you can set $k=5$ in softmax regression, and also have a fifth, \"none of the above,\" class.)\n",
    "\n",
    "If however your categories are has\\_vocals, dance, soundtrack, pop, then the\n",
    "classes are not mutually exclusive; for example, there can be a piece of pop\n",
    "music that comes from a soundtrack and in addition has vocals.  In this case, it\n",
    "would be more appropriate to build 4 binary logistic regression classifiers. \n",
    "This way, for each new musical piece, your algorithm can separately decide whether\n",
    "it falls into each of the four categories.\n",
    "\n",
    "Now, consider a computer vision example, where you're trying to classify images into\n",
    "three different classes.  (i) Suppose that your classes are indoor\\_scene,\n",
    "outdoor\\_urban\\_scene, and outdoor\\_wilderness\\_scene.  Would you use softmax regression\n",
    "or three logistic regression classifiers?  (ii) Now suppose your classes are\n",
    "indoor\\_scene, black\\_and\\_white\\_image, and image\\_has\\_people.  Would you use softmax\n",
    "regression or multiple logistic regression classifiers?\n",
    "\n",
    "In the first case, the classes are mutually exclusive, so a softmax regression\n",
    "classifier would be appropriate.  In the second case, it would be more appropriate to build\n",
    "three separate logistic regression classifiers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical Example (MNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import newaxis, c_, mat\n",
    "from numpy.linalg import *\n",
    "from scipy import optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our cost function is now\n",
    "\\begin{align*}\n",
    "J(\\theta) = - \\frac{1}{m} \\left[ \\sum_{i=1}^{m} \\sum_{j=1}^{k} 1\\left\\{y^{(i)} = j\\right\\} \\log \\frac{e^{\\theta^{(j)\\top} x^{(i)}}}{\\sum_{l=1}^k e^{ \\theta^{(l)\\top} x^{(i)} }}  \\right]\n",
    "              + \\frac{\\lambda}{2} \\sum_{i=1}^k \\sum_{j=0}^n \\theta_{ij}^2\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax_cost(theta, num_classes, input_size, lambda_, X, y):\n",
    "    '''\n",
    "    :param theta:\n",
    "    :param num_classes: the number of classes\n",
    "    :param input_size: the size N of input vector(num of features)\n",
    "    :param lambda_: weight decay parameter\n",
    "    :param data: the M x N input matrix, where each column corresponds\n",
    "                 a single test set\n",
    "    :param labels: an M x 1 matrix containing the labels for the input data\n",
    "    '''\n",
    "    \n",
    "    m, n = X.shape\n",
    "    theta = theta.reshape(n, num_classes)\n",
    "    theta_data = X.dot(theta)\n",
    "    theta_data = theta_data - np.max(theta_data) # overparameterized\n",
    "    \n",
    "    prob_data = np.exp(theta_data) / np.exp(theta_data).sum()\n",
    "    \n",
    "    indicator = np.zeros((m, theta.shape[1]), dtype=np.int8)\n",
    "    for i, idx in enumerate(y):\n",
    "        indicator[i][idx] = 1\n",
    "  \n",
    "    value = np.sum((indicator.T).dot(np.log(prob_data)))\n",
    "\n",
    "    cost = (-1 / m) * value + (lambda_ / 2) * np.sum(theta * theta)\n",
    "\n",
    "    grad = (-1 / m) * X.T.dot(indicator - prob_data) + lambda_ * theta\n",
    "\n",
    "    return cost, grad.flatten()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our hypothesis now estimates the class label probabilities as.\n",
    "\\begin{align*}\n",
    "P(y^{(i)} = k | x^{(i)} ; \\theta)\n",
    "&= \\frac{\\exp(\\theta^{(k)\\top} x^{(i)})}{\\sum_{j=1}^K \\exp(\\theta^{(j)\\top} x^{(i)})}.\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax_predict(model, data):\n",
    "    # model - model trained using softmaxTrain\n",
    "    # data - the N x M input matrix, where each column data(:, i) corresponds to\n",
    "    #        a single test set\n",
    "    #\n",
    "    # Your code should produce the prediction matrix\n",
    "    # pred, where pred(i) is argmax_c P(y(c) | x(i)).\n",
    "\n",
    "    opt_theta, input_size, num_classes = model\n",
    "    opt_theta = opt_theta.reshape(input_size, num_classes)\n",
    "\n",
    "    prod = data.dot(opt_theta)\n",
    "    pred = np.exp(prod) / np.sum(np.exp(prod), axis=0)\n",
    "    pred = pred.argmax(axis=0)\n",
    "\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derivative of this\n",
    "new definition of $J(\\theta)$.  One can show that the derivative is:\n",
    "\\begin{align*}\n",
    "\\nabla_{\\theta^{(j)}} J(\\theta) = - \\frac{1}{m} \\sum_{i=1}^{m}{ \\left[ x^{(i)} ( 1\\{ y^{(i)} = j\\}  - P(y^{(i)} = j | x^{(i)}; \\theta) ) \\right]  } + \\lambda \\theta^{(j)}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_train(input_size, num_classes, lambda_, data, labels, options={'maxiter': 400, 'disp': True}):\n",
    "    #softmaxTrain Train a softmax model with the given parameters on the given\n",
    "    # data. Returns softmaxOptTheta, a vector containing the trained parameters\n",
    "    # for the model.\n",
    "    #\n",
    "    # input_size: the size of an input vector x^(i)\n",
    "    # num_classes: the number of classes\n",
    "    # lambda_: weight decay parameter\n",
    "    # input_data: an N by M matrix containing the input data, such that\n",
    "    #            inputData(:, c) is the cth input\n",
    "    # labels: M by 1 matrix containing the class labels for the\n",
    "    #            corresponding inputs. labels(c) is the class label for\n",
    "    #            the cth input\n",
    "    # options (optional): options\n",
    "    #   options.maxIter: number of iterations to train for\n",
    "\n",
    "    # Initialize theta randomly\n",
    "    theta = 0.005 * np.random.randn(num_classes * input_size)\n",
    "\n",
    "    J = lambda x: softmax_cost(x, num_classes, input_size, lambda_, data, labels)\n",
    "\n",
    "    result = optimize.minimize(J, theta, method='L-BFGS-B', jac=True, options=options)\n",
    "\n",
    "    # Return optimum theta, input size & num classes\n",
    "    opt_theta = result.x\n",
    "\n",
    "    return opt_theta, input_size, num_classes\n",
    "  \n",
    "def featureNormalize(X):\n",
    "    X_norm = X.A\n",
    " \n",
    "    mu = X_norm.mean(axis=0)\n",
    "    X_norm -= mu # broadcasting\n",
    " \n",
    "    sigma = X_norm.std(axis=0)\n",
    "    X_norm /= sigma\n",
    " \n",
    "    return mat(X_norm), mu, sigma\n",
    "\n",
    "def compute_gradient(J, theta): \n",
    "    # theta: a vector of parameters\n",
    "    # J: a function that outputs a real-number. Calling y = J(theta) will return the\n",
    "    # function value at theta.\n",
    "    epsilon = 0.0001\n",
    "\n",
    "    gradient = np.zeros(theta.shape)\n",
    "    for i in range(theta.shape[0]):\n",
    "        theta_epsilon_plus = np.array(theta, dtype=np.float64)\n",
    "        theta_epsilon_plus[i] = theta[i] + epsilon\n",
    "        theta_epsilon_minus = np.array(theta, dtype=np.float64)\n",
    "        theta_epsilon_minus[i] = theta[i] - epsilon\n",
    "\n",
    "        gradient[i] = (J(theta_epsilon_plus)[0] - J(theta_epsilon_minus)[0]) / (2 * epsilon)\n",
    "        if i % 100 == 0:\n",
    "            print (\"Computing gradient for input:\", i)\n",
    "\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "477.719057553\n",
      "(1, 7840)\n"
     ]
    }
   ],
   "source": [
    "#Main Function\n",
    "if __name__ == '__main__':\n",
    "    # =================== Part 0: load dataset\n",
    "\n",
    "    f = pd.read_csv('./Data/mnist_train.csv') \n",
    "    #f.head()\n",
    "    data = np.mat(f)\n",
    "    #x = np.reshape(x1, (28,28))\n",
    "    #plt.imshow(x, cmap='gray')\n",
    "    \n",
    "    X = mat(data[:, 1:])\n",
    "    y = c_[data[:, 0]]\n",
    "    #idx = (y == 0).nonzero()[:1]\n",
    "    #y[idx,0] = 10\n",
    "    m, n = X.shape\n",
    " \n",
    "    # =================== Part 1: Feature Normalization\n",
    "\n",
    "    #X, mu, sigma = featureNormalize(X)\n",
    " \n",
    "    # =================== Part 2: Initialise constants and parameters\n",
    "\n",
    "    featureSize = n # Size of input vector (MNIST images are 28x28)\n",
    "    k = 10     # Number of classes (MNIST images fall into 10 classes)\n",
    "\n",
    "    lambda_ = 1e-4 # Weight decay parameter\n",
    "    \n",
    "    theta = np.random.randn(n * k, 1) * 0.005\n",
    "    \n",
    "    # =================== Part 3: Implement softmaxCost\n",
    "\n",
    "    cost, grad = softmax_cost(theta, k, featureSize, lambda_, X, y)\n",
    "    \n",
    "    # =================== Part 5: Gradient checking\n",
    "    #\n",
    "    #  As with any learning algorithm, you should always check that your\n",
    "    #  gradients are correct before learning the parameters.\n",
    "    #\n",
    "     debug = False\n",
    "\n",
    "    images = X\n",
    "    labels = y\n",
    "\n",
    "    if debug:\n",
    "        J = lambda theta_: softmax.softmax_cost(theta_, num_classes, featureSize, lambda_, X, y)\n",
    "\n",
    "        num_grad = gradient.compute_gradient(J, theta)\n",
    "\n",
    "        # Use this to visually compare the gradients side by side\n",
    "        print(num_grad, grad)\n",
    "\n",
    "        # Compare numerically computed gradients with the ones obtained from backpropagation\n",
    "        diff = np.linalg.norm(num_grad - grad) / np.linalg.norm(num_grad + grad)\n",
    "        print (diff)\n",
    "        print (\"Norm of the difference between numerical and analytical num_grad (should be < 1e-7)\\n\\n\")\n",
    "\n",
    "    # ================= Part 6: Learning parameters\n",
    "    #\n",
    "    #  Once you have verified that your gradients are correct,\n",
    "    #  you can start training your softmax regression code using softmaxTrain\n",
    "\n",
    "    options_ = {'maxiter': 100, 'disp': True}\n",
    "    opt_theta, input_size, num_classes = softmax_train(input_size, num_classes, lambda_, input_data, labels, options_)\n",
    "\n",
    "    # ================ Part 7: Testing\n",
    "    #\n",
    "    #  You should now test your model against the test images.\n",
    "    #  To do this, you will first need to write softmaxPredict\n",
    "    #  (in softmaxPredict.m), which should return predictions\n",
    "    #  given a softmax model and the input data.\n",
    "    \n",
    "    file_ = pd.read_csv('./Data/mnist_test.csv') \n",
    "    data = np.mat(file_)\n",
    "    \n",
    "    test_images = mat(data[:, 1:])\n",
    "    test_labels = c_[data[:, 0]]\n",
    "\n",
    "    predictions = softmax_predict((opt_theta, input_size, num_classes), test_images)\n",
    "    print \"Accuracy: {0:.2f}%\".format(100 * np.sum(predictions == test_labels, dtype=np.float64) / test_labels.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
